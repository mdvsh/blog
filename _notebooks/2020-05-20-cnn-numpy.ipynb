{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Convolutional Model Building Blocks\"\n",
    "> \"An attempt at implementing a CNN to from scratch using NumPy to better understand its working.\"\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [dl]\n",
    "- image: images/backprop.png\n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Gist hosted [here](https://gist.github.com/PseudoCodeNerd/ae94cc895a1c9302853306abdf99a49b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### TODO:\n",
    "- Convolution Functions\n",
    "     - Zero padding\n",
    "     - Convolve window\n",
    "     - Forward convolution\n",
    "     - Backward Convolution\n",
    "- Pooling Function\n",
    "     - Forward pool\n",
    "     - Mask creation\n",
    "     - Value distribution\n",
    "     - Backward Pool\n",
    "---\n",
    "\n",
    "<center>Basic structure of CNN</center>\n",
    "\n",
    "![image.png](img/model_cnn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Padding\n",
    "To add zeros around the image matrix to prevent loss of features due to scaling down after one step of a convolution.\n",
    "*Same Convolution*: padding such that h-w of original image preserved after one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, p):\n",
    "    \"\"\"\n",
    "    params\n",
    "    X: (n, nH, nW, nC) dims array representing a batch of images\n",
    "    p: int, amount of padding around each image\n",
    "    \"\"\"\n",
    "    pad_width = ((0, 0), (p, p), (p, p), (0, 0))\n",
    "    X_p = np.pad(X, pad_width, mode='constant', constant_values=(0, 0))\n",
    "    return X_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of padding some sample data and demonstrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =\n",
      " (1, 3, 3, 2)\n",
      "x_p.shape =\n",
      " (1, 7, 7, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f33b5942bd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.randn(1, 3, 3, 2)\n",
    "x_p = zero_pad(x, 2)\n",
    "print (\"X.shape =\\n\", x.shape)\n",
    "print (\"x_p.shape =\\n\", x_p.shape)\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('X: Original Image Matrix')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('X_p: Padded Image Matrix')\n",
    "axarr[1].imshow(x_p[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "1. `conv_one_part()`\n",
    "    -   TODO:  Take input volume (matrix by no. of channels) and **convolve** filter against it to output new volume with features (hopefully) identified.\n",
    "\n",
    "`conv_one_part()` will apply convolution to a part of the given image matrix (X) of dimensions filter_h x filter_w, taking steps of value *stride* after each iteration of the function.\n",
    "To be implemented in the next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_one_part(a_slice, W, b):\n",
    "    \"\"\"\n",
    "    params\n",
    "    m_slice: slice of input matrix; dims --> (f_h, f_w, nC_prev)\n",
    "    W: Weight params contained in a window; dims --> (f_h, f_w, nC_prev)\n",
    "    b: Bias params contained in a window; dims --> (1, 1, 1) : scalar\n",
    "    \"\"\"\n",
    "    Z = float(np.add(np.sum(np.multiply(a_slice, W)), b))\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `forward_conv()`\n",
    "    - TODO: Take multiple filters and convolve all of them on the input. Stack 2D Matrix outputs to produce output volume giving result of a single forward pass of convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_conv(A_prev, W, b, hparams):\n",
    "    \"\"\"\n",
    "    params\n",
    "    A_prev: previous layer activation; dims --> (n, nH, nW, nC_prev)\n",
    "    W: Weight params contained in a window; dims --> (f_h, f_w, nC_prev, nC)\n",
    "    b: Bias params contained in a window; dims --> (1, 1, 1, nC) : scalar\n",
    "    hparams: dict containing values for stride and padding\n",
    "    \n",
    "    return\n",
    "    Z: conv step output; dims --> (n, nH, nW, nC)\n",
    "    cache: for calculating derivatives in backward_conv()\n",
    "    \"\"\"\n",
    "    # Init: Dimensions, hparams\n",
    "    (n, nH_prev, nW_prev, nC_prev) = np.shape(A_prev)\n",
    "    (f, f, nC_prev, nC) = np.shape(W)\n",
    "    s = hparams['stride']\n",
    "    pad = hparams['padding']\n",
    "    nH = int((nH_prev-f+2*pad)/s)+1\n",
    "    nW = int((nW_prev-f+2*pad)/s)+1\n",
    "    Z = np.zeros((n, nH, nW, nC))\n",
    "    \n",
    "    # Applying padding to prev layer activation\n",
    "    A_prev_p = zero_pad(A_prev, pad)\n",
    "    \n",
    "    # Loop (Vectorization >>>>>>> Loops) to apply convolution operation.\n",
    "    for i in range(n):\n",
    "        a_prev_p = A_prev_p[i, :, :, :]\n",
    "        for h in range(nH):\n",
    "            for w in range(nW):\n",
    "                vert1_f, vert2_f = h*s, h*s+f\n",
    "                hori1_f, hori2_f = w*s, w*s+f\n",
    "                for c in range (nC):\n",
    "                    # slice\n",
    "                    a_slice = a_prev_p[vert1_f:vert2_f, hori1_f:hori2_f, :]\n",
    "                    Z[i, h, w, c] = conv_one_part(a_slice, W[:, :, :, c], b[:, :, :, c])\n",
    "    # for backward_conv()               \n",
    "    cache = (A_prev, W, b, hparams)\n",
    "#     assert(Z.shape == (n, nH, nW, nC))\n",
    "\n",
    "    return (Z, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing one iteration of `forward_conv()` on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =\n",
      " -0.1282614539128993\n",
      "Z[3,2,1] =\n",
      " [ 4.98925312 -0.12934609  6.77487928 -6.44934224  1.80531313  8.75470928\n",
      " -2.85387942 -2.65858316]\n",
      "cache_conv[0][1][2][3] = [-0.9970198  -0.10679399  1.45142926 -0.61803685]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,8,8,4)\n",
    "W = np.random.randn(3,3,4,8) # channels of A_prev and W has to be the same (here, 4)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparams = {\"padding\" : 2, \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = forward_conv(A_prev, W, b, hparams)\n",
    "print(\"Z's mean =\\n\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\\n\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "Pooling operation after convolution to keep strong features by taking the maximum / average value contained in a sub-matrix of dims of the filter. Pooling helps reduce computation, as well as helps make feature detectors more invariant to its position in the input. \n",
    "`forward_pool()` implments a forward pass of the pooling layer. By default, *maxpool*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pool(A_prev, hparams, mode=\"maxpool\"):\n",
    "    \"\"\"\n",
    "    params\n",
    "    A_prev: previous layer activation; dims --> (n, nH, nW, nC_prev)\n",
    "    hparams: dict containing values for filter_size and padding\n",
    "    mode: pooling to perform; default --> maxpool\n",
    "    \n",
    "    return\n",
    "    A: pool step output; dims --> (n, nH, nW, nC)\n",
    "    cache: for calculating derivatives in backward_pool()\n",
    "    \"\"\"\n",
    "    # Init: Dimensions, hparams\n",
    "    (n, nH_prev, nW_prev, nC_prev) = np.shape(A_prev)\n",
    "    s = hparams['stride']\n",
    "    fs = hparams['filt_size']\n",
    "    nH = int((nH_prev-fs)/s)+1\n",
    "    nW = int((nW_prev-fs)/s)+1\n",
    "    nC = nC_prev\n",
    "    A = np.zeros((n, nH, nW, nC))\n",
    "    \n",
    "    # Loop (Vectorization >>>>>>> Loops) to apply pooling operation.\n",
    "    for i in range(n):\n",
    "        for h in range(nH):\n",
    "            for w in range(nW):\n",
    "                for c in range(nC):\n",
    "                    vert1_f, vert2_f = h*s, h*s+fs\n",
    "                    hori1_f, hori2_f = w*s, w*s+fs\n",
    "                    a_slice = A_prev[i, vert1_f:vert2_f, hori1_f:hori2_f, c]\n",
    "                    if mode == 'maxpool': A[i, h, w, c] = np.max(a_slice)\n",
    "                    elif mode == 'avrgpool': A[i, h, w, c] = np.mean(a_slice)\n",
    "    # for backward_conv()               \n",
    "    cache = (A_prev, hparams)\n",
    "#     assert(A.shape == (n, nH, nW, nC))\n",
    "    return (A, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing one iteration of `forward_pool(mode='maxpool')` on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prev.shape = (1, 5, 5, 3)\n",
      "A = \n",
      " [[[[ 1.62434536 -0.61175641 -0.52817175]\n",
      "   [-1.07296862  0.86540763 -2.3015387 ]\n",
      "   [ 1.74481176 -0.7612069   0.3190391 ]\n",
      "   [-0.24937038  1.46210794 -2.06014071]\n",
      "   [-0.3224172  -0.38405435  1.13376944]]\n",
      "\n",
      "  [[-1.09989127 -0.17242821 -0.87785842]\n",
      "   [ 0.04221375  0.58281521 -1.10061918]\n",
      "   [ 1.14472371  0.90159072  0.50249434]\n",
      "   [ 0.90085595 -0.68372786 -0.12289023]\n",
      "   [-0.93576943 -0.26788808  0.53035547]]\n",
      "\n",
      "  [[-0.69166075 -0.39675353 -0.6871727 ]\n",
      "   [-0.84520564 -0.67124613 -0.0126646 ]\n",
      "   [-1.11731035  0.2344157   1.65980218]\n",
      "   [ 0.74204416 -0.19183555 -0.88762896]\n",
      "   [-0.74715829  1.6924546   0.05080775]]\n",
      "\n",
      "  [[-0.63699565  0.19091548  2.10025514]\n",
      "   [ 0.12015895  0.61720311  0.30017032]\n",
      "   [-0.35224985 -1.1425182  -0.34934272]\n",
      "   [-0.20889423  0.58662319  0.83898341]\n",
      "   [ 0.93110208  0.28558733  0.88514116]]\n",
      "\n",
      "  [[-0.75439794  1.25286816  0.51292982]\n",
      "   [-0.29809284  0.48851815 -0.07557171]\n",
      "   [ 1.13162939  1.51981682  2.18557541]\n",
      "   [-1.39649634 -1.44411381 -0.50446586]\n",
      "   [ 0.16003707  0.87616892  0.31563495]]]]\n",
      "\n",
      "Pooling type : Max Pooling\n",
      "A.shape = (1, 2, 2, 3)\n",
      "A =\n",
      " [[[[1.74481176 0.90159072 1.65980218]\n",
      "   [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.6924546  2.18557541]]]]\n",
      "\n",
      "Pooling type : Average Pooling\n",
      "A.shape = (1, 2, 2, 3)\n",
      "A =\n",
      " [[[[-0.03010467 -0.00324021 -0.33629886]\n",
      "   [ 0.12893444  0.22242847  0.1250676 ]]\n",
      "\n",
      "  [[-0.38268052  0.23257995  0.6259979 ]\n",
      "   [-0.09525515  0.268511    0.46605637]]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(1, 5, 5, 3)\n",
    "hparams = {\"stride\" : 2, \"filt_size\": 3}\n",
    "\n",
    "print('A_prev.shape = ' + str(A_prev.shape))\n",
    "print(\"A = \\n\", A_prev)\n",
    "print()\n",
    "A, cache = forward_pool(A_prev, hparams)\n",
    "print(\"Pooling type : Max Pooling\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)\n",
    "print()\n",
    "A, cache = forward_pool(A_prev, hparams, mode = \"avrgpool\")\n",
    "print(\"Pooling type : Average Pooling\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer - Backward Pass\n",
    "*Note: $dZ_{hw}$ is a scalar corresponding to the gradient of the cost with respect to the output of the conv layer Z at the hth row and wth column (corresponding to the dot product taken at the ith stride left and jth stride down).*\n",
    "\n",
    "Further, We need to compute :\n",
    "1. $dA$ (w.r.t cost for a certain filter $W_{c}$)\n",
    "     $$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "     \n",
    "     - Notice, how everytime the same filter is multiplied by a different derivative of cost w.r.t output of conv layer Z ($dZ$)\n",
    "2. $dW$ (derivative of one filter w.r.t to the loss)\n",
    "    $$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "    - Where, $a_{slice}$ is the slice of original matrix used to generate activation $Z_{ij}$. This follows from the fact that the filter matrix can also learn (from backprop) optimal values.   \n",
    "3. $db$ ( w.r.t to the cost of a certain filter $dW_{c}$)\n",
    "    $$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "    \n",
    "    - summing over all the gradients of the conv output (Z) with respect to the cost.\n",
    "    \n",
    "#### `backward_conv()` : To implement the backward propagation for a convolution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_conv(dZ, cache):\n",
    "    \"\"\"\n",
    "    params\n",
    "    dZ: gradient of cost w.r.t conv layer output (Z); dims --> (n, nH, nW, nC)\n",
    "    cache: cache of values needed for backward_conv(); i.e. output of forward_conv()\n",
    "    \n",
    "    returns\n",
    "    see above (markdown)\n",
    "    \"\"\"\n",
    "    # Init: Dimensions, hparams\n",
    "    (A_prev, W, b, hparams) = cache\n",
    "    (n, nH_prev, nW_prev, nC_prev) = np.shape(A_prev)\n",
    "    (f, f, nC_prev, nC) = np.shape(W)\n",
    "    s = hparams['stride']\n",
    "    pad = hparams['padding']\n",
    "    (n, nH, nW, nC) = np.shape(dZ)\n",
    "    \n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    dW = np.zeros_like(W)\n",
    "    db = np.zeros_like(b)\n",
    "    \n",
    "    A_prev_p = zero_pad(A_prev, pad)\n",
    "    dA_prev_p  = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    # Loop (Vectorization >>>>>>> Loops) for backward convolution step.\n",
    "    for i in range(n):\n",
    "        a_prev_p = A_prev_p[i, :, :, :]\n",
    "        da_prev_p = dA_prev_p[i, :, :, :]\n",
    "        for h in range(nH):\n",
    "            for w in range(nW):\n",
    "                for c in range(nC):\n",
    "                    vert1_f, vert2_f = h*s, h*s+f\n",
    "                    hori1_f, hori2_f = w*s, w*s+f\n",
    "                    # slice\n",
    "                    a_slice = a_prev_p[vert1_f:vert2_f, hori1_f:hori2_f, :]\n",
    "                    # updating gradients\n",
    "                    da_prev_p[vert1_f:vert2_f, hori1_f:hori2_f, :] += W[:, :, :, c] * dZ[i, h, w, c]\n",
    "                    dW[:, :, :, c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:, :, :, c] += dZ[i, h, w, c]\n",
    "        dA_prev[i, :, :, :] = da_prev_p[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "#     assert(dA_prev.shape == (m, nH_prev, nW_prev, nC_prev))\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing `backward_conv()` on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = -0.9683520023516613\n",
      "dW_mean = -3.028451139022465\n",
      "db_mean = 41.04575496729348\n"
     ]
    }
   ],
   "source": [
    "# We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
    "# which we'll use to test the conv_backward function\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,6) # six filters\n",
    "b = np.random.randn(1,1,1,6)\n",
    "hparameters = {\"padding\" : 2, \"stride\": 2}\n",
    "Z, cache_conv = forward_conv(A_prev, W, b, hparameters)\n",
    "# Testing backward_conv()\n",
    "dA, dW, db = backward_conv(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer - Backward Pass\n",
    "Although, pooling layer has no learnable parameters for backpropagation, we still need to go through the pooling layer to complete gradient computation for layers that come before pooling layer.\n",
    "\n",
    "To compute backward pooling, we would need a function `mask_window()` to create a matrix which keeps track of where the maximum of the matrix is. \n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 && 2 \\\\\n",
    "3 && 4\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "0 && 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "#### But, why do we keep track of the position of the max? \n",
    "\n",
    "It's because this is the input value that ultimately influenced the output, and therefore the cost. Backprop is computing gradients with respect to the cost, so anything that influences the ultimate cost should have a non-zero gradient. So, backprop will \"propagate\" the gradient back to this particular input value that had influenced the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_mask(x):\n",
    "    \"\"\"\n",
    "    params\n",
    "    x: input matrix to be masked\n",
    "    \n",
    "    returns\n",
    "    m_x: masked matrix, same dims as x, 1 / True at max elem position\n",
    "    \"\"\"\n",
    "    m_x = (x == np.max(x))\n",
    "    return m_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[5 8 9]\n",
      " [5 0 0]\n",
      " [1 7 6]]\n",
      "m_z =  [[False False  True]\n",
      " [False False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randint(10, size=(3, 3))\n",
    "print('x = ', x)\n",
    "print('m_x = ', max_mask(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also need a similar mask function for average pooling as well. \n",
    "\n",
    "In case of average pooling, every elem of the sliced (window) matrix has equal influence on the output *unlike* max pooling where maximum influence is by the largest element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avrg_mask(x, dims):\n",
    "    \"\"\"\n",
    "    params\n",
    "    x: input scalar to be masked\n",
    "    dims: dims of array we want to distribute x to. \n",
    "    \n",
    "    returns\n",
    "    m_x: masked matrix, same dims as x with x distributed among it\n",
    "    \"\"\"\n",
    "    (nH, nW) = dims\n",
    "    avg  = x/(nH*nW)\n",
    "    m_x = avg * np.ones((nH, nW))\n",
    "    return m_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value to be Distributed:  25 \n",
      "Distributed / Average Mask:\n",
      " [[2.77777778 2.77777778 2.77777778]\n",
      " [2.77777778 2.77777778 2.77777778]\n",
      " [2.77777778 2.77777778 2.77777778]]\n"
     ]
    }
   ],
   "source": [
    "print('Value to be Distributed: ', 25,'\\nDistributed / Average Mask:\\n', avrg_mask(25, (3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our helper functions in place, we can proceed towards writing our final function of the day `backward_pool()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pool(dA, cache, mode='maxpool'):\n",
    "    \"\"\"\n",
    "    params\n",
    "    dA: gradient of cost w.r.t output of pooling layer; dims --> like A\n",
    "    cache: cache output from forward step of pooling layer; contains inputs and hparams\n",
    "    moode: max/average pool\n",
    "    \n",
    "    returns\n",
    "    dA_prev: gradient of cost w.r.t input of pooling layer; dims --> like A_prev\n",
    "    \"\"\"\n",
    "    # Init; Dimensions and hparams\n",
    "    (A_prev, hparams) = cache\n",
    "    s = hparams['stride']\n",
    "    fs = hparams['filt_size']\n",
    "    (n, nH, nW, nC) = np.shape(dA)\n",
    "    (n, nH_prev, nW_prev, nC_prev) = np.shape(A_prev)\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    \n",
    "    # Loop (Vectorization >>>>>>> Loops) for backward pooling step.\n",
    "    for i in range(n):\n",
    "        a_prev = A_prev[i, :, :, :]\n",
    "        for h in range(nH):\n",
    "            for w in range(nW):\n",
    "                for c in range(nC):\n",
    "                    vert1_f, vert2_f = h*s, h*s+fs\n",
    "                    hori1_f, hori2_f = w*s, w*s+fs\n",
    "                    if mode=='maxpool':\n",
    "                        a_prev_slice = a_prev[vert1_f:vert2_f, hori1_f:hori2_f, c]\n",
    "                        # create mask from a_prev_slice\n",
    "                        mask = max_mask(a_prev_slice)\n",
    "                        dA_prev[i, vert1_f:vert2_f, hori1_f:hori2_f, c] += mask * dA[i, h, w, c]\n",
    "                    elif mode=='avrgpool':\n",
    "                        da = dA[i, h, w, c]\n",
    "                        dims = (fs, fs)\n",
    "                        dA_prev[i, vert1_f:vert2_f, hori1_f:hori2_f, c] += avrg_mask(da, dims)\n",
    "                        \n",
    "#     assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing `backward_pool()` on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooling type : Max Pooling\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Pooling type : Average Pooling\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"filt_size\": 2}\n",
    "A, cache = forward_pool(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = backward_pool(dA, cache, mode = \"maxpool\")\n",
    "print(\"Pooling type : Max Pooling\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = backward_pool(dA, cache, mode = \"avrgpool\")\n",
    "print(\"Pooling type : Average Pooling\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "With this, we have completed all the basic building ~~blocks~~ functions required to build a Convolutional Model.\n",
    "\n",
    "While coding this out, I was able to greatly increase my understanding about the mathematical working beneath both convolution and pooling operations. \n",
    "\n",
    "With deeplearning libraries such as PyTorch and Tensorflow making such things a breeze, (only 10 lines of pytorch code to do all things I've done in this notebook) it's not practical to define CNN models from scratch using numpy since tensors (numpy arrays + GPU support) are the go-to. Still, this endeavour turned out to be extremely knowledgable and *EPIC*.\n",
    "\n",
    "### *kthnxbye*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
