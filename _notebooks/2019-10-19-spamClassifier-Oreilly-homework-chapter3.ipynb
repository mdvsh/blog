{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Email spam classifier from scratch\"\n",
    "> \"Soln to 3.3 | Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow:\"\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [ml]\n",
    "- image: images/spamham.png\n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n",
    "## Chapter 3: Classification\n",
    "### Exercise: Question 3\n",
    "\n",
    "**Problem Statement**:\n",
    "Build a spam classifier ( a more challenging experience)\n",
    "* Download examples of spam and ham from Apaches SpamAssasin's Public DataSet.\n",
    "* Unzip data and familiarize yourself with data format.\n",
    "* Split data-sets into training and test.\n",
    "* Write a data preparation pipeline to convert each email into a feature vector. The pipeline should transform email into a (sparse) vector that indicates presence or absence of each possible word. \n",
    "* You may add hyperparameters to prep. pipeline to control whether or not to strip of email header, convert mail to lowercase, remove punctuation, replace URLS with \"url\", replace all numbers with \"NUM\" or do stemming.\n",
    "\n",
    "{Optional}, try out several classifiers and see if you can build a great spam classifier, with high recall and precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Official Data Desc.](http://spamassassin.apache.org/old/publiccorpus/readme.html) \n",
    "  - spam: 500 spam messages, all received from non-spam-trap sources.\n",
    "\n",
    "  - easy_ham: 2500 non-spam messages.  These are typically quite easy to\n",
    "    differentiate from spam, since they frequently do not contain any spammish\n",
    "    signatures (like HTML etc).\n",
    "\n",
    "  - hard_ham: 250 non-spam messages which are closer in many respects to\n",
    "    typical spam: use of HTML, unusual HTML markup, coloured text,\n",
    "    \"spammish-sounding\" phrases etc.\n",
    "\n",
    "  - easy_ham_2: 1400 non-spam messages.  A more recent addition to the set.\n",
    "\n",
    "  - spam_2: 1397 spam messages.  Again, more recent.\n",
    "\n",
    "Total count: 6047 messages, with about a 31% spam ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "down_path = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "ham_url = down_path + \"20030228_easy_ham.tar.bz2\"\n",
    "spam_url = down_path + \"20030228_spam.tar.bz2\"\n",
    "spam_path = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_spam_data(spam_url=spam_url, spam_path=spam_path):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=spam_path)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_directory = os.path.join(spam_path, \"easy_ham\")\n",
    "spam_directory = os.path.join(spam_path, \"spam\")\n",
    "ham_filenames = [name for name in sorted(os.listdir(ham_directory)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(spam_directory)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(ham_filenames))\n",
    "print(len(spam_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using email module and policy function (in email) in python to parse mails\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "def get_mails(is_spam, file, spam_path=spam_path):\n",
    "    if is_spam:\n",
    "        directory = \"spam\"\n",
    "    else:\n",
    "        directory = \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, directory, file), \"rb\") as f:\n",
    "              return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "ham_emails = [get_mails(is_spam=False, file=name) for name in ham_filenames]\n",
    "spam_emails = [get_mails(is_spam=True, file=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< >\n",
      "> I downloaded a driver from the nVidia website and installed it using RPM.\n",
      "> Then I ran Sax2 (as was recommended in some postings I found on the net),\n",
      "but\n",
      "> it still doesn't feature my video card in the available list. What next?\n",
      "\n",
      "\n",
      "hmmm.\n",
      "\n",
      "Peter.\n",
      "\n",
      "Open a terminal and as root type\n",
      "lsmod\n",
      "you want to find a module called\n",
      "NVdriver.\n",
      "\n",
      "If it isn't loaded then load it.\n",
      "#insmod NVdriver.o\n",
      "Oh and ensure you have this module loaded on boot.... else when you reboot\n",
      "you might be in for a nasty surprise.\n",
      "\n",
      "Once the kernel module is loaded\n",
      "\n",
      "#vim /etc/X11/XF86Config\n",
      "\n",
      "in the section marked\n",
      "Driver I have \"NeoMagic\"\n",
      "you need to have\n",
      "Driver \"nvidia\"\n",
      "\n",
      "Here is part of my XF86Config\n",
      "\n",
      "Also note that using the card you are using you 'should' be able to safely\n",
      "use the FbBpp 32 option .\n",
      "\n",
      "Section \"Module\"\n",
      " Load  \"extmod\"\n",
      " Load  \"xie\"\n",
      " Load  \"pex5\"\n",
      " Load  \"glx\"\n",
      " SubSection \"dri\"    #You don't need to load this Peter.\n",
      "  Option     \"Mode\" \"666\"\n",
      " EndSubSection\n",
      " Load  \"dbe\"\n",
      " Load  \"record\"\n",
      " Load  \"xtrap\"\n",
      " Load  \"speedo\"\n",
      " Load  \"type1\"\n",
      "EndSection\n",
      "\n",
      "#Plus the Modelines for your monitor should be singfinicantly different.\n",
      "\n",
      "Section \"Monitor\"\n",
      " Identifier   \"Monitor0\"\n",
      " VendorName   \"Monitor Vendor\"\n",
      " ModelName    \"Monitor Model\"\n",
      " HorizSync    28.00-35.00\n",
      " VertRefresh  43.00-72.00\n",
      "        Modeline \"800x600\" 36 800 824 896 1024 600 601 603 625\n",
      " Modeline \"1024x768\" 49 1024 1032 1176 1344 768 771 777 806\n",
      "EndSection\n",
      "\n",
      "Section \"Device\"\n",
      "\n",
      " Identifier  \"Card0\"\n",
      " Driver      \"neomagic\" #Change this to \"nvidia\"... making sure the modules\n",
      "are in the correct path\n",
      " VendorName  \"Neomagic\" # \"Nvidia\"\n",
      " BoardName   \"NM2160\"\n",
      " BusID       \"PCI:0:18:0\"\n",
      "EndSection\n",
      "\n",
      "Section \"Screen\"\n",
      " Identifier \"Screen0\"\n",
      " Device     \"Card0\"\n",
      " Monitor    \"Monitor0\"\n",
      " DefaultDepth 24\n",
      " SubSection \"Display\"\n",
      "  Depth     1\n",
      " EndSubSection\n",
      " SubSection \"Display\"\n",
      "  Depth     4\n",
      " EndSubSection\n",
      " SubSection \"Display\"\n",
      "  Depth     8\n",
      " EndSubSection\n",
      " SubSection \"Display\"\n",
      "  Depth     15\n",
      " EndSubSection\n",
      " SubSection \"Display\"\n",
      "  Depth     16\n",
      " EndSubSection\n",
      " SubSection \"Display\"\n",
      "  Depth     24\n",
      "  #FbBpp   32 #Ie you should be able lto uncomment this line\n",
      "  Modes   \"1024x768\" \"800x600\" \"640x480\" # And add in higher resulutions as\n",
      "desired.\n",
      " EndSubSection\n",
      "EndSection\n",
      "\n",
      "\n",
      "-- \n",
      "Irish Linux Users' Group: ilug@linux.ie\n",
      "http://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\n",
      "List maintainer: listmaster@linux.ie\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[42].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help wanted.  We are a 14 year old fortune 500 company, that is\n",
      "growing at a tremendous rate.  We are looking for individuals who\n",
      "want to work from home.\n",
      "\n",
      "This is an opportunity to make an excellent income.  No experience\n",
      "is required.  We will train you.\n",
      "\n",
      "So if you are looking to be employed from home with a career that has\n",
      "vast opportunities, then go:\n",
      "\n",
      "http://www.basetel.com/wealthnow\n",
      "\n",
      "We are looking for energetic and self motivated people.  If that is you\n",
      "than click on the link and fill out the form, and one of our\n",
      "employement specialist will contact you.\n",
      "\n",
      "To be removed from our link simple go to:\n",
      "\n",
      "http://www.basetel.com/remove.html\n",
      "\n",
      "\n",
      "7749doNL1-136DfsE5701lGxl2-486pAKM7127JwoR4-054PCfq9499xMtW0-594hucS91l66\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[42].get_content().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some emails are actually multipart, with images and attachments. Let's look at the various types of structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()\n",
    "    \n",
    "from collections import Counter\n",
    "\n",
    "def structure_count(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2408),\n",
       " ('multipart(text/plain, application/pgp-signature)', 66),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure_count(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure_count(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that spam has got quite a lot HTML and plain text (either together or individualy)\n",
    "ham mails are often plain text and are signed using PGP (spam isn't). Concretely, email structure\n",
    "appears to be an important feature in classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path --> <bill@bluemail.dk>\n",
      "Delivered-To --> zzzz@localhost.spamassassin.taint.org\n",
      "Received --> from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 98B7343F99\tfor <zzzz@localhost>; Mon, 26 Aug 2002 10:12:43 -0400 (EDT)\n",
      "Received --> from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Mon, 26 Aug 2002 15:12:43 +0100 (IST)\n",
      "Received --> from smtp.easydns.com (smtp.easydns.com [205.210.42.30])\tby webnote.net (8.9.3/8.9.3) with ESMTP id TAA11952;\tFri, 23 Aug 2002 19:49:56 +0100\n",
      "From --> bill@bluemail.dk\n",
      "Received --> from bluemail.dk (klhtnet.klht.pvt.k12.ct.us [206.97.9.2])\tby smtp.easydns.com (Postfix) with SMTP\tid 754E52CFFB; Fri, 23 Aug 2002 14:49:52 -0400 (EDT)\n",
      "Reply-To --> bill@bluemail.dk\n",
      "Message-ID --> <003d35d40cab$6883b2c8$6aa10ea4@khnqja>\n",
      "To --> byrt5@hotmail.com\n",
      "Subject --> FORTUNE 500 COMPANY HIRING, AT HOME REPS.\n",
      "MiME-Version --> 1.0\n",
      "Content-Type --> text/plain; charset=\"iso-8859-1\"\n",
      "X-Priority --> 3 (Normal)\n",
      "X-MSMail-Priority --> Normal\n",
      "X-Mailer --> Microsoft Outlook Express 6.00.2462.0000\n",
      "Importance --> Normal\n",
      "Date --> Fri, 23 Aug 2002 14:49:52 -0400\n",
      "Content-Transfer-Encoding --> 8bit\n"
     ]
    }
   ],
   "source": [
    "#email_headers\n",
    "for header, value in spam_emails[42].items():\n",
    "    print(header,\"-->\",value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a networking guy would assure you that this in-fact is an overload of info which can be used for effective classification however, i gotta read some of these headers up to get more background info on how spam affects the headers... \n",
    "For now lets just figure stuff out from the \"Subject\" header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FORTUNE 500 COMPANY HIRING, AT HOME REPS.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails[42][\"Subject\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo\n",
    "#- Convert HTML to plain text (using BS4 or regex)\n",
    "import re \n",
    "from html import unescape\n",
    "\n",
    "def htmlTOtext(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><body><center>\n",
      "\n",
      "<table bgcolor=\"663399\" border=\"2\" width=\"999\" cellspacing=\"0\" cellpadding=\"0\">\n",
      "  <tr>\n",
      "    <td colspan=\"3\" width=\"999\"> <hr><font color=\"yellow\"> \n",
      "<center>\n",
      "<font size=\"7\"> \n",
      "<br><center><b>Get 12 FREE VHS or DVDs! </b><br>\n",
      "<table bgcolor=\"white\" border=\"2\" width=\"500\">\n",
      "  <tr>    <td>\n",
      " <font size=\"7\"> <font color=\"003399\"><center>Click <a href=\"http://www.bozomber.com/porno/index.html\"> HERE For Details!</a>\n",
      "<font size=\"5\"><br>\n",
      "</td></tr></table> <br> \n",
      "\n",
      "<table bgcolor=\"#CCFF33\" border=\"2\" width=\"600\">\n",
      "  <tr>    <td><center><center><font size=\"6\"><font color=\"6633CC\"><br>\n",
      "We Only Have HIGH QUALITY <br>Porno Movies to Choose From!<br><br>\n",
      " \n",
      " \"This is a <i>VERY SPECIAL, LIMITED TIME OFFER</i>.\"<br><br> Get up to 12 DVDs absolutely FREE,<br> with<a href=\"http://www.bozomber.com/porno/index.html\"> NO COMMITMENT!</a> \n",
      " <br><br>\n",
      "There's <b>no better deal anywhere</b>.<br>\n",
      "There's <i>no catches</i> and <i>no gimmicks</i>. <br>You only pay for the shipping,<br> and the DVDs  ...\n"
     ]
    }
   ],
   "source": [
    "#checking htmlTOtext\n",
    "htmlSPAM = []\n",
    "for email in X_train[y_train==1]:\n",
    "    if email_structure(email) == \"text/html\":\n",
    "        htmlSPAM.append(email)\n",
    "sampleSPAM = htmlSPAM[5]\n",
    "print(sampleSPAM.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Get 12 FREE VHS or DVDs!\n",
      "  Click  HYPERLINK  HERE For Details!\n",
      "We Only Have HIGH QUALITY Porno Movies to Choose From!\n",
      " \"This is a VERY SPECIAL, LIMITED TIME OFFER.\" Get up to 12 DVDs absolutely FREE, with HYPERLINK  NO COMMITMENT!\n",
      "There's no better deal anywhere.\n",
      "There's no catches and no gimmicks. You only pay for the shipping, and the DVDs are absolutely free!\n",
      "Take a Peak at our HYPERLINK   Full Catalog!\n",
      " High quality cum filled titles such as:\n",
      " HYPERLINK  500 Oral Cumshots 5\n",
      "Description: 500 Oral Cum Shots! I need hot jiz on my face! Will you cum in my mouth?\n",
      " Dozens of Dirty Hardcore titles such as:\n",
      " HYPERLINK  Amazing Penetrations No. 17\n",
      "Description: 4 full hours of amazing penetrations with some of the most beautiful women in porn!\n",
      " From our \"Sexiest Innocent Blondes\" collections:\n",
      " HYPERLINK  Audition Tapes\n",
      "Description: Our girls go from cute, young and innocent, to screaming sex goddess\n",
      " beggin' to have massive cocks in their tight, wet pussies and asses!\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "print(htmlTOtext(sampleSPAM.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Great! Now let's write a function that takes an email as input and returns its content as plain text, whatever its format is:\n",
    "def emailTOtext(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return htmlTOtext(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSFW Below \n",
    "not me, but the data is NSFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Get 12 FREE VHS or DVDs!\n",
      "  Click  HYPERLINK  HERE For Details!\n",
      "We Only Have HIGH QUALITY Porno Movi ...\n"
     ]
    }
   ],
   "source": [
    "print(emailTOtext(sampleSPAM)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's do more text preprocessing, technically-> stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations --> comput\n",
      "Computation --> comput\n",
      "Computing --> comput\n",
      "Computed --> comput\n",
      "Compute --> comput\n",
      "Compulsive --> compuls\n",
      "Technology --> technolog\n",
      "Convulated --> convul\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stemmer = nltk.PorterStemmer()\n",
    "for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\",\"Technology\",\"Convulated\"):\n",
    "        print(word, \"-->\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's also do as the problem statement says and change all URLS to \"URL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['talktosharmadhav.netlify.com', 'wikipedia.com', 'www.github.com/pseudocodenerd', 'https://www.youtube.com/watch?v=_7QRpuhz-90']\n"
     ]
    }
   ],
   "source": [
    "import urlextract\n",
    "urlextractor = urlextract.URLExtract()\n",
    "#try\n",
    "print(urlextractor.find_urls(\"My personal website is talktosharmadhav.netlify.com and I like to surf wikipedia.com and keep my code on www.github.com/pseudocodenerd I just watched this https://www.youtube.com/watch?v=_7QRpuhz-90\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lol nice, it works\n",
    "<br>Now, let's put all this together into a text transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class dopeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        \n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = emailTOtext(email)\n",
    "            \n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', str(text))#regexIStough!!\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            if self.replace_urls and urlextractor is not None:\n",
    "                urls = list(set(urlextractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)  \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1, 'R': 1})\n",
      " Counter({'the': 11, 'of': 9, 'and': 8, 'all': 3, 'christian': 3, 'by': 3, 'jefferson': 2, 'I': 2, 'have': 2, 'superstit': 2, 'one': 2, 'on': 2, 'been': 2, 'ha': 2, 'half': 2, 'to': 2, 'rogueri': 2, 'teach': 2, 'jesu': 2, 'some': 1, 'interest': 1, 'quot': 1, 'http': 1, 'www': 1, 'postfun': 1, 'com': 1, 'pfp': 1, 'worboi': 1, 'html': 1, 'thoma': 1, 'examin': 1, 'known': 1, 'word': 1, 'do': 1, 'not': 1, 'find': 1, 'in': 1, 'our': 1, 'particular': 1, 'redeem': 1, 'featur': 1, 'they': 1, 'are': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mytholog': 1, 'million': 1, 'innoc': 1, 'men': 1, 'women': 1, 'children': 1, 'sinc': 1, 'introduct': 1, 'burnt': 1, 'tortur': 1, 'fine': 1, 'imprison': 1, 'what': 1, 'effect': 1, 'thi': 1, 'coercion': 1, 'To': 1, 'make': 1, 'world': 1, 'fool': 1, 'other': 1, 'hypocrit': 1, 'support': 1, 'error': 1, 'over': 1, 'earth': 1, 'six': 1, 'histor': 1, 'american': 1, 'john': 1, 'E': 1, 'remsburg': 1, 'letter': 1, 'william': 1, 'short': 1, 'again': 1, 'becom': 1, 'most': 1, 'pervert': 1, 'system': 1, 'that': 1, 'ever': 1, 'shone': 1, 'man': 1, 'absurd': 1, 'untruth': 1, 'were': 1, 'perpetr': 1, 'upon': 1, 'a': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul': 1, 'first': 1, 'great': 1, 'corrupt': 1})]\n"
     ]
    }
   ],
   "source": [
    "sampleX = X_train[:2]\n",
    "sampleXwordcount = dopeTransformer().fit_transform(sampleX)\n",
    "print(sampleXwordcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the the word counts with us, we need to vectorize them for use in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class dopeVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocab_size =1000):\n",
    "        self.vocab_size = vocab_size\n",
    "            \n",
    "    def fit(self, X, y=None):#builds the vocabulary (an ordered list of the most common words)\n",
    "        countT = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                countT[word]+=min(count, 10)\n",
    "        mostCommon = countT.most_common()[:self.vocab_size]\n",
    "        self.mostCommon = mostCommon\n",
    "        self.vocab = {word: index + 1 for index, (word, count) in enumerate(mostCommon)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        R=[]; C=[]; Data=[]\n",
    "        for r, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                R.append(r)\n",
    "                C.append(self.vocab.get(word,0))\n",
    "                Data.append(count)\n",
    "        return csr_matrix((Data, (R, C)), shape=(len(X), self.vocab_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t6\n",
      "  (1, 0)\t115\n",
      "  (1, 1)\t11\n",
      "  (1, 2)\t9\n",
      "  (1, 3)\t8\n",
      "  (1, 4)\t3\n",
      "  (1, 5)\t3\n",
      "{'the': 1, 'of': 2, 'and': 3, 'all': 4, 'christian': 5}\n"
     ]
    }
   ],
   "source": [
    "sampleVectorX = dopeVectorTransformer(vocab_size=5)\n",
    "sampleVectors = sampleVectorX.fit_transform(sampleXwordcount)\n",
    "print(sampleVectors)\n",
    "sampleVectors.toarray()\n",
    "print(sampleVectorX.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_nice_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do this on the entire data now we have tests it\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pre_processing = Pipeline([(\"email_to_word_count\", dopeTransformer()),\n",
    "                          (\"wordcount_to_vector\", dopeVectorTransformer()),\n",
    "                          ])\n",
    "X_final = pre_processing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\shekh\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.985, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.985, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................. , score=0.99125, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9870833333333332"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finally\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = LogisticRegression(random_state=42)\n",
    "score = cross_val_score(model, X_final, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 98.7; Dope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}